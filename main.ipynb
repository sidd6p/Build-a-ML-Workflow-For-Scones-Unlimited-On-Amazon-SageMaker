{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy and monitor a machine learning workflow for Image Classification\n",
    "\n",
    "1. __Data Staging__\n",
    "    1. Extract the data from a hosting service\n",
    "    2. Transform it into a usable shape and format\n",
    "    3. Explore the data\n",
    "    4. Filter the objects to find the label numbers for Bicycle and Motorcycles\n",
    "    5. Convert the object into the dataframe\n",
    "    6. Save the data to the local machine\n",
    "    7. Load it into a production system\n",
    "    \n",
    "2. __Model Training__\n",
    "    1. Create metadata for image classification on SageMaker\n",
    "    2. Upload metadata to S3 using `boto3`\n",
    "    3. Get algorithm using ECR image\n",
    "    4. Create estimator\n",
    "    5. Add hyperparameters to the estimator\n",
    "    6. Add model inputs\n",
    "    7. Fit the model\n",
    "\n",
    "3. __Getting ready to deploy__\n",
    "    1. Creating data capture\n",
    "    2. Model deployment and creating the endpoint\n",
    "    3. Instantiating a predictor\n",
    "    4. Making Prediction\n",
    "    \n",
    "4. __Draft Lambdas and Step Function Workflow__\n",
    "    1. Lambad 1: Serialize target data from S3\n",
    "    2. Lambad 2: Classification of image\n",
    "    3. Lambda 3: Check for confidence threshold\n",
    "    \n",
    "5. __Testing and Evaluation__\n",
    "    1. Generating multiple test cases (event input for lambda 1)\n",
    "    2. Pulling in the JSONLines data from your inferences\n",
    "    3. Plotting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Data Staging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A. Extract the data from the hosting service__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the cell below, define a function `extract_cifar_data` that extracts python version of the CIFAR-100 dataset. \n",
    "- The CIFAR dataaset is open source and generously hosted by the University of Toronto at: https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz \n",
    "- This will download a zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
    "file_name = \"cifar.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "def extract_cifar_data(url, file_name):\n",
    "    \"\"\"\n",
    "    Downloads the CIFAR-100 dataset from the specified URL and saves it as a gzipped file.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL from which the CIFAR-100 dataset will be downloaded.\n",
    "    filename (str, optional): The name of the file where the downloaded dataset will be saved.\n",
    "                              Defaults to \"cifar.tar.gz\".\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_name):\n",
    "        # Send a GET request to the specified URL to get the dataset content\n",
    "        r = requests.get(url)\n",
    "\n",
    "        # Open the file in binary write mode and write the dataset content into it\n",
    "        with open(file_name, \"wb\") as file_context:\n",
    "            file_context.write(r.content)\n",
    "\n",
    "        # Function execution completes, no return value necessary\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_cifar_data(download_url, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B. Transform the data into a usable shape and format__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The zip file has 3 compressed form of python serialized object (serialized dict using pickle).\n",
    "- So we will extract the zip and then unpickle them into python dict.\n",
    "- Create a new folder `cifar-100-python`, containing `meta`, `test`, and `train` files, these are serialized python object. \n",
    "- These files are `pickles` and the [CIFAR homepage](https://www.cs.toronto.edu/~kriz/cifar.html) provides a simple script that can be used to load them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "def unzip_data(file_name, mode=\"r:gz\"):\n",
    "    \n",
    "    if not os.path.exists(\"cifar-100-python\"):\n",
    "        # Open the gzipped tar file in read mode\n",
    "        with tarfile.open(file_name, mode) as tar:\n",
    "            # Extract all the contents of the tar file\n",
    "            tar.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def de_serialize_pickle_data():\n",
    "    # Open and load the metadata file\n",
    "    with open(\"./cifar-100-python/meta\", \"rb\") as f:\n",
    "        # Load metadata using pickle, specifying 'bytes' encoding\n",
    "        dataset_meta = pickle.load(f, encoding='bytes')\n",
    "\n",
    "    # Open and load the test data file\n",
    "    with open(\"./cifar-100-python/test\", \"rb\") as f:\n",
    "        # Load test data using pickle, specifying 'bytes' encoding\n",
    "        dataset_test = pickle.load(f, encoding='bytes')\n",
    "\n",
    "    # Open and load the training data file\n",
    "    with open(\"./cifar-100-python/train\", \"rb\") as f:\n",
    "        # Load training data using pickle, specifying 'bytes' encoding\n",
    "        dataset_train = pickle.load(f, encoding='bytes')\n",
    "    \n",
    "    return dataset_meta, dataset_test, dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unzip_data(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_meta, dataset_test, dataset_train = de_serialize_pickle_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C. Explore the data__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All unpicked objects are Python dictionaries.\n",
    "- The train dictionary has 5 keys:\n",
    "    1. `b'filenames'`\n",
    "    2. `b'batch_label'`\n",
    "    3. `b'fine_labels'`\n",
    "    4. `b'coarse_labels'`\n",
    "    5. `b'data'`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- The test dictionary has 5 keys:\n",
    "    1. `b'filenames'`\n",
    "    2. `b'batch_label'`\n",
    "    3. `b'fine_labels'`\n",
    "    4. `b'coarse_labels'`\n",
    "    5. `b'data'`\n",
    "    \n",
    " \n",
    " \n",
    " - The meta dictionary has 2 keys:\n",
    "    1. `b'fine_label_names'`\n",
    "    2. `b'coarse_label_names'`\n",
    "    3. `b'fine_labels'`\n",
    "    4. `b'coarse_labels'`\n",
    "    5. `b'data'`\n",
    "    \n",
    "    \n",
    "    \n",
    "- As documented on the homepage, `b'data'` 50000 contains rows of 3073 unsigned integers, representing three channels (red, green, and blue) for one 32x32 pixel image per row. So total = 32x32x3 = 3072 per index.\n",
    "\n",
    "\n",
    "- `dataset_train[b'fine_labels']` contains the label for the image, this label act as index in `dataset_meta[b'fine_label_names']`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(dataset_train),type(dataset_train[b'filenames']), type(dataset_train[b'fine_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset_meta), type(dataset_test), type(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.keys(), dataset_test.keys(), dataset_meta.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train[b'data'], len(dataset_train[b'data']), len(dataset_train[b'data'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform one of our images. \n",
    "Using python, we can stack these channels into a 32x32x3 array, and save it as a PNG file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_single_image(image_array):\n",
    "    channel_size = 32*32\n",
    "    # Load the dataset_train and select the first row\n",
    "    row = image_array\n",
    "\n",
    "    # Split the row into red, green, and blue channels\n",
    "    red, green, blue = row[0:channel_size], row[channel_size:channel_size*2], row[channel_size*2:channel_size*3]\n",
    "    \n",
    "    # Reshape the channel data into 32x32 arrays\n",
    "    red = red.reshape(32, 32)\n",
    "    green = green.reshape(32, 32)\n",
    "    blue = blue.reshape(32, 32)\n",
    "    print(red.shape, red[0].shape)\n",
    "\n",
    "    # Combine the individual channels into a 32x32x3 image using NumPy's dstack function\n",
    "    combined = np.dstack((red, green, blue))\n",
    "    \n",
    "    plt.imshow(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_single_image(dataset_train[b'data'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like a cow! Let's check the label. `dataset_meta` contains label names in order, and `dataset_train` has a list of labels for each row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index in `dataset_meta[b'fine_label_names']` are actually the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train[b'fine_labels'][0:5], dataset_meta[b'fine_label_names'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_meta[b'fine_label_names'][dataset_train[b'fine_labels'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_train[b'filenames'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Taurus\" is the name of a subspecies of cattle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__D. Filter the objects to find the label numbers for Bicycle and Motorcycles__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CIFAR-100 has image for 100 classes, we need to filter the label that are accosicated with bicycle and motorcycle class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for  index, label_name in enumerate(dataset_meta[b'fine_label_names']):\n",
    "    if label_name in [b'bicycle',b'motorcycle']:\n",
    "        print( f\"Label Name: {label_name},  Label Number: {index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__E. Convert the object into the dataframe__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(range(len(dataset_train[b'filenames'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Construct the dataframe for training data\n",
    "df_train = pd.DataFrame({\n",
    "    \"filenames\": dataset_train[b'filenames'],\n",
    "    \"labels\": dataset_train[b'fine_labels'],\n",
    "    \"row\": range(len(dataset_train[b'filenames'])) # for adding the index column\n",
    "})\n",
    "\n",
    "# Drop all rows from df_train where label is not 8 or 48\n",
    "df_train = df_train.loc[df_train[\"labels\"].isin([8,48])]\n",
    "\n",
    "# Decode df_train.filenames so they are regular strings\n",
    "#  b'bike_s_000682.png' ==> bike_s_000682.png\n",
    "df_train[\"filenames\"] = df_train[\"filenames\"].apply(lambda x: x.decode(\"utf-8\"))\n",
    "\n",
    "# Construct the dataframe for testing data\n",
    "df_test = pd.DataFrame({\n",
    "    \"filenames\": dataset_test[b'filenames'],\n",
    "    \"labels\": dataset_test[b'fine_labels'],\n",
    "    \"row\": range(len(dataset_test[b'filenames']))\n",
    "})\n",
    "\n",
    "# Drop all rows from df_test where label is not 8 or 48\n",
    "df_test = df_test.loc[df_test[\"labels\"].isin([8,48])]\n",
    "\n",
    "# Decode df_test.filenames so they are regular strings\n",
    "df_test[\"filenames\"] = df_test[\"filenames\"].apply(lambda x: x.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `df_train` and `df_test` have filename, lable, and index from original data dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__F. Save the data to the local machine__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./train\n",
    "!mkdir ./test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Grabbing the image data:\n",
    "\n",
    "```python\n",
    "dataset_train[b'data'][0]\n",
    "```\n",
    "\n",
    "2. A simple idiom for stacking the image data into the right shape\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "np.dstack((\n",
    "    row[0:1024].reshape(32,32),\n",
    "    row[1024:2048].reshape(32,32),\n",
    "    row[2048:].reshape(32,32)\n",
    "))\n",
    "```\n",
    "\n",
    "3. A simple `matplotlib` utility for saving images\n",
    "\n",
    "```python\n",
    "plt.imsave(path+row['filenames'], target)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(image_data_row_num , image_filename, target_folder_path, images_dataset):\n",
    "    \"\"\"A function for saving images from the dataset to the provided target path folders \n",
    "    \n",
    "    Arguments:\n",
    "    image_data_row_num  -- image data row that needs to be saved into a folder\n",
    "    image_filename            -- filename with which the image needs to be saved\n",
    "    target_folder_path        --  target folder path where image needs to be saved\n",
    "    images_dataset             -- original images dataset containing the data for the given images\n",
    "    \n",
    "    \"\"\"\n",
    "    #Grab the image data in row-major form\n",
    "    img_data =  images_dataset[b'data'][image_data_row_num]\n",
    "    \n",
    "    # Consolidated stacking/reshaping from earlier\n",
    "    target = np.dstack((\n",
    "        img_data[0:1024].reshape(32,32),\n",
    "        img_data[1024:2048].reshape(32,32),\n",
    "        img_data[2048:].reshape(32,32)\n",
    "    ))\n",
    "    \n",
    "    # Save the image\n",
    "    try:\n",
    "        image_file_path = os.path.join(target_folder_path, image_filename)\n",
    "        plt.imsave(image_file_path, target)\n",
    "    except e:\n",
    "        return f\"Error Saving {image_filename} to folder {target_folder_path} \\n Error: {e}  \"\n",
    "    # Return any signal data you want for debugging\n",
    "    return f\"Successfully saved {image_filename} to folder {target_folder_path}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Saving all the Train dataset images\n",
    "for df_row in df_train.itertuples():\n",
    "    print(save_images(df_row.row, df_row.filenames,\"./train\", dataset_train))\n",
    "\n",
    "#Saving all the Test dataset images\n",
    "for df_row in df_test.itertuples():\n",
    "    print(save_images(df_row.row, df_row.filenames,\"./test\", dataset_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__G. Load the data__\n",
    "\n",
    "- Now we can load the data into S3.\n",
    "- Using the sagemaker SDK grab the current region, execution role, and bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "\n",
    "bucket= \"sidd0final0project0bucket\"\n",
    "print(\"Default Bucket: {}\".format(bucket))\n",
    "\n",
    "region = \"us-east-1\"\n",
    "print(\"AWS Region: {}\".format(region))\n",
    "\n",
    "role =\"arn:aws:iam::271232843618:role/service-role/AmazonSageMaker-ExecutionRole-20231021T211247\"\n",
    "print(\"RoleArn: {}\".format(role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this data we can easily sync your data up into S3!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DEFAULT_S3_BUCKET\"] = bucket\n",
    "!aws s3 sync ./train s3://${DEFAULT_S3_BUCKET}/train/\n",
    "!aws s3 sync ./test s3://${DEFAULT_S3_BUCKET}/test/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Image Classification, Sagemaker [also expects metadata](https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html) e.g. in the form of TSV files with labels and filepaths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A. Creating metadata for image classification on SageMaker__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_metadata_file(df, prefix):\n",
    "    df[\"s3_path\"] = df[\"filenames\"]\n",
    "    df[\"labels\"] = df[\"labels\"].apply(lambda x: 0 if x==8 else 1)\n",
    "    return df[[\"row\", \"labels\", \"s3_path\"]].to_csv(\n",
    "        f\"{prefix}.lst\", sep=\"\\t\", index=False, header=False\n",
    "    )\n",
    "    \n",
    "to_metadata_file(df_train.copy(), \"train\")\n",
    "to_metadata_file(df_test.copy(), \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B. Uploading metadata to S3 using boto3__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 # for interacting with AWS services\n",
    "\n",
    "# Upload files\n",
    "boto3.Session().resource('s3').Bucket(\n",
    "    bucket).Object('train.lst').upload_file('./train.lst')\n",
    "boto3.Session().resource('s3').Bucket(\n",
    "    bucket).Object('test.lst').upload_file('./test.lst')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C. Getting algorithm using ECR image__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the `bucket` and `region` info we can get the latest prebuilt container to run our training job, and define an output location on our s3 bucket for the model. \n",
    "- Use the `image_uris` function from the SageMaker SDK to retrieve the latest `image-classification` image below\n",
    "- The retrieve method is used to get the ECR (Elastic Container Registry) URI for a pre-built SageMaker Docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_image = sagemaker.image_uris.retrieve(\"image-classification\", region=region, version=\"latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__D. Creating estimator__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = f\"s3://{bucket}/models/image_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Estimator\n",
    "img_classifier_model=sagemaker.estimator.Estimator(\n",
    "    image_uri = algo_image,\n",
    "    role= role,\n",
    "    instance_count = 1,\n",
    "    instance_type = 'ml.p3.2xlarge',\n",
    "    output_path = s3_output_location,\n",
    "    sagemaker_session = sagemaker.Session()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__E. Adding hyperparameters to the estimator__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_classifier_model.set_hyperparameters(\n",
    "    image_shape='3,32,32',\n",
    "    num_classes=2,\n",
    "    num_training_samples= df_train.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__F. Adding model inputs__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import Rule, rule_configs\n",
    "from sagemaker.session import TrainingInput\n",
    "model_inputs = {\n",
    "        \"train\": sagemaker.inputs.TrainingInput(\n",
    "            s3_data=f\"s3://{bucket}/train/\",\n",
    "            content_type=\"application/x-image\"\n",
    "        ),\n",
    "        \"validation\": sagemaker.inputs.TrainingInput(\n",
    "            s3_data=f\"s3://{bucket}/test/\",\n",
    "            content_type=\"application/x-image\"\n",
    "        ),\n",
    "        \"train_lst\": sagemaker.inputs.TrainingInput(\n",
    "            s3_data=f\"s3://{bucket}/train.lst\",\n",
    "            content_type=\"application/x-image\"\n",
    "        ),\n",
    "        \"validation_lst\": sagemaker.inputs.TrainingInput(\n",
    "            s3_data=f\"s3://{bucket}/test.lst\",\n",
    "            content_type=\"application/x-image\"\n",
    "        )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__G. Fitting the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_classifier_model.fit(inputs=model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If all goes well, you'll end up with a model topping out above `.8` validation accuracy. With only 1000 training samples in the CIFAR dataset, that's pretty good.\n",
    "- We could definitely pursue data augmentation & gathering more samples to help us improve further, but for now let's proceed to deploy our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Getting ready to deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A. Creating data capture__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig #  to configure data capture for endpoints.\n",
    "\n",
    "# Define the destination S3 URI for data capture\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True, # input/output payloads will be captured.\n",
    "    sampling_percentage=100, # 100 means capturing data from every request.\n",
    "    destination_s3_uri=f\"s3://{bucket}/data_capture\" #  will be saved in this S3 bucket\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B. Model deployment and creating the endpoint__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = img_classifier_model.deploy(\n",
    "    instance_type=\"ml.m5.xlarge\",  \n",
    "    initial_instance_count=1,\n",
    "    data_capture_config=data_capture_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint = deployment.endpoint_name\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C. Instantiating a predictor__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sagemaker.predictor.Predictor(\n",
    "    endpoint_name=endpoint, \n",
    "    sagemaker_session=sagemaker.Session()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__D. Making Prediction__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import IdentitySerializer\n",
    "import base64\n",
    "\n",
    "predictor.serializer = IdentitySerializer(\"image/png\")\n",
    "with open(\"./test/bicycle_s_001789.png\", \"rb\") as f:\n",
    "    payload = f.read()\n",
    "\n",
    "    \n",
    "inference = predictor.predict(payload, initial_args={'ContentType': 'application/x-image'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `inference` object is an array of two values, the predicted probability value for each of your classes (bicycle and motorcycle respectively.) \n",
    "- So, for example, a value of `b'[0.91, 0.09]'` indicates the probability of being a bike is 91% and being a motorcycle is 9%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Draft Lambdas and Step Function Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A. Lambad 1: Serialize target data from S3__\n",
    "\n",
    "- Download an image file from an S3 bucket\n",
    "- Convert the image data into a base64-encoded format\n",
    "- Return this data along with some metadata as a response\n",
    "\n",
    "- event input\n",
    "```\n",
    "{\n",
    "    \"s3_key\": \"test/bicycle_s_000030.png\",\n",
    "    \"s3_bucket\": \"ml-flow-sidd\"\n",
    "}\n",
    "```\n",
    "\n",
    "- The lambda_handler function is defined, which is the entry point for the Lambda function. \n",
    "- This function takes two parameters: event and context. \n",
    "- The event parameter contains information about the event that triggered the Lambda function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import base64 #  for encoding and decoding data in base64 forma\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"A function to serialize target data from S3\"\"\"\n",
    "    \n",
    "    key = event[\"s3_key\"]\n",
    "    bucket = event[\"s3_bucket\"]\n",
    "    \n",
    "    # Download the data from s3 to /tmp/image.png\n",
    "    boto3.resource('s3').Bucket(bucket).download_file(key, \"/tmp/image.png\")\n",
    "    \n",
    "    with open(\"/tmp/image.png\", \"rb\") as f:\n",
    "        image_data = base64.b64encode(f.read())\n",
    "\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': {\n",
    "            \"image_data\": image_data,\n",
    "            \"s3_bucket\": bucket,\n",
    "            \"s3_key\": key,\n",
    "            \"inferences\": []\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B. Lambad 2: Classification of image__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creates a SageMaker runtime client using boto3.client('runtime.sagemaker') to interact with the SageMaker endpoint.\n",
    "- Send an image (in base64-encoded format) to an Amazon SageMaker endpoint for inference \n",
    "- Return the inference results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import base64\n",
    "\n",
    "ENDPOINT_NAME = endpoint\n",
    "\n",
    "# We will be using the AWS's lightweight runtime solution to invoke an endpoint.\n",
    "runtime = boto3.client('runtime.sagemaker')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    image = base64.b64decode(event['body']['image_data'])\n",
    "    \n",
    "    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n",
    "                                       ContentType='image/png',\n",
    "                                       Body=image)\n",
    "    \n",
    "    event[\"inferences\"] = json.loads(response['Body'].read().decode('utf-8'))\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': event\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C. Lambda 3: Check for confidence threshold__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the threshold for confidence\n",
    "THRESHOLD = 0.8 \n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    inferences =  event['body']['inferences']\n",
    "    \n",
    "    # Check if any values in our inferences are above THRESHOLD\n",
    "    meets_threshold = any(confidence >= THRESHOLD for confidence in inferences)\n",
    "    \n",
    "\n",
    "    # If our threshold is met, pass our data back out of the Step Function,\n",
    "    # else, end the Step Function with an error\n",
    "    if meets_threshold:\n",
    "        pass\n",
    "    else:\n",
    "        raise Exception(\"THRESHOLD_CONFIDENCE_NOT_MET\")\n",
    "    \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps(event)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Testing and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A. Generating multiple test cases (event input for lambda 1)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "\n",
    "def generate_test_case():\n",
    "    # Setup s3 in boto3\n",
    "    s3 = boto3.resource('s3')\n",
    "    \n",
    "    # Randomly pick from sfn or test folders in our bucket\n",
    "    objects = s3.Bucket(bucket).objects.filter(Prefix=\"test/\")\n",
    "    \n",
    "    # Grab any random object key from that folder!\n",
    "    obj = random.choice([x.key for x in objects])\n",
    "    \n",
    "    return json.dumps({\n",
    "        \"image_data\": \"\",\n",
    "        \"s3_bucket\": bucket,\n",
    "        \"s3_key\": obj\n",
    "    })\n",
    "generate_test_case()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B. Pulling in the JSONLines data from your inferences__\n",
    "\n",
    "    1. Downloading the capture (in JSONL format)\n",
    "    2. Extracting JSONL format in dict\n",
    "    3. Loading inferences along with timestamp\n",
    "    4. Visulaizing (the input images, the resulting inferences, and the timestamps.)\n",
    "    \n",
    " - The data are in JSONLines format, where multiple valid JSON objects are stacked on top of eachother in a single `jsonl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader #  utilities for downloading data from Amazon S3.\n",
    "\n",
    "# In S3 your data will be saved to a datetime-aware path\n",
    "data_path = \"s3://sidd0final0project0bucket/data_capture/image-classification-2023-10-25-10-26-31-173/AllTraffic/2023/10/26/14/\"\n",
    "\n",
    "S3Downloader.download(data_path, \"captured_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jsonlines\n",
    "\n",
    "import jsonlines\n",
    "import os\n",
    "\n",
    "# List the file names we downloaded\n",
    "file_handles = os.listdir(\"./captured_data\")\n",
    "\n",
    "# Dump all the data into an array\n",
    "json_data = []\n",
    "for jsonl in file_handles:\n",
    "    with jsonlines.open(f\"./captured_data/{jsonl}\") as f:\n",
    "        json_data.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how we'll get our data\n",
    "def simple_getter(obj):\n",
    "    inferences = obj[\"captureData\"][\"endpointOutput\"][\"data\"]\n",
    "    timestamp = obj[\"eventMetadata\"][\"inferenceTime\"]\n",
    "    return json.loads(inferences), timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C. Plotting the results__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Populate the data for the x and y axis\n",
    "x = []\n",
    "y = []\n",
    "for obj in json_data:\n",
    "    inference, timestamp = simple_getter(obj)\n",
    "    \n",
    "    y.append(max(inference))\n",
    "    x.append(timestamp)\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(x, y, c=['r' if k<. else 'b' for k in y ])\n",
    "plt.axhline(y=0.8, color='g', linestyle='--')\n",
    "plt.ylim(bottom=.6)\n",
    "\n",
    "# Add labels\n",
    "plt.ylabel(\"Confidence\")\n",
    "plt.suptitle(\"Observed Recent Inferences\", size=14)\n",
    "plt.title(\"Pictured with confidence threshold for production use\", size=10)\n",
    "\n",
    "# Give it some pizzaz!\n",
    "plt.style.use(\"Solarize_Light2\")\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(x, y)    \n",
    "plt.xlabel(\"Inference\")\n",
    "plt.ylabel(\"Confidence Level\")\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
